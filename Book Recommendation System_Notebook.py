# -*- coding: utf-8 -*-
"""Final Project_Melshita Ardia Kirana

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19lEA5p5SGq9n0vqXvDoV2IefkGX_PgXD

## Project Overview
Pada proyek ini, kita akan membangun sistem rekomendasi buku menggunakan pendekatan Collaborative Filtering. Dataset yang digunakan adalah [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset?select=Ratings.csv) dari Kaggle yang berisi informasi buku, pengguna, dan rating yang diberikan pengguna terhadap buku.

Tujuan utama dari proyek ini adalah merekomendasikan buku berdasarkan riwayat interaksi pengguna sebelumnya.


## Business Understanding

Kembangkan sebuah sistem rekomendasi buku untuk menjawab permasalahan berikut:

- Bagaimana membangun sistem rekomendasi buku yang dipersonalisasi menggunakan teknik collaborative filtering berbasis deep learning?

- Bagaimana sistem dapat memprediksi dan merekomendasikan buku lain yang kemungkinan besar disukai pengguna berdasarkan data rating sebelumnya?

- Bagaimana sistem ini dapat memberikan nilai tambah bagi pengguna dalam menemukan buku-buku yang belum pernah mereka baca sebelumnya?

Untuk menjawab pertanyaan tersebut, sistem rekomendasi ini dikembangkan dengan tujuan atau goals sebagai berikut:

- Menghasilkan rekomendasi buku yang dipersonalisasi untuk setiap pengguna berdasarkan pola interaksi dan rating historis.

- Membangun model rekomendasi menggunakan pendekatan embedding neural network untuk collaborative filtering.

- Menyediakan prediksi rating yang akurat sebagai dasar pengambilan keputusan dalam sistem rekomendasi.

# Data Understanding

Menginstal library penting yang digunakan untuk pemrosesan numerik, machine learning, dan rekomendasi.
"""

!pip install numpy==1.23.5
!pip install tensorflow==2.12
!pip install jax==0.4.6 jaxlib==0.4.6
!pip install scikit-surprise

"""Menginstal CLI untuk mengakses dataset dari Kaggle."""

!pip install -q kaggle

"""Mengunggah token API Kaggle untuk mengakses data,  serta mengunduh dan mengekstrak dataset dari Kaggle."""

from google.colab import files

# Mengupload file kaggle.json dari komputer lokal
files.upload()

import os
import shutil

os.makedirs('/root/.kaggle', exist_ok=True)

shutil.copy('kaggle.json', '/root/.kaggle/kaggle.json')

os.chmod('/root/.kaggle/kaggle.json', 600)

# Download dataset Book Recommendation
!kaggle datasets download -d arashnic/book-recommendation-dataset

# Ekstrak file zip
import zipfile

with zipfile.ZipFile("book-recommendation-dataset.zip", 'r') as zip_ref:
    zip_ref.extractall("book_data")

"""Membaca ketiga file CSV utama ke DataFrame."""

# Load data
import pandas as pd

books = pd.read_csv('book_data/Books.csv', encoding='latin-1')
users = pd.read_csv('book_data/Users.csv', encoding='latin-1')
ratings = pd.read_csv('book_data/Ratings.csv', encoding='latin-1')

"""Menampilkan informasi struktur DataFrame."""

# Tampilkan info dasar
print("Books")
print(books.info())
print("\nUsers")
print(users.info())
print("\nRatings")
print(ratings.info())

"""Menyajikan informasi mengenai ukuran dataset."""

print('Jumlah data buku: ', len(books.ISBN.unique()))
print('Jumlah data user: ', len(users['User-ID'].unique()))
print('Jumlah data rating: ', len(ratings))
print('Jumlah rating eksplisit (rating > 0): ', len(ratings[ratings['Book-Rating'] > 0]))
print('Jumlah rating implisit (rating == 0): ', len(ratings[ratings['Book-Rating'] == 0]))

"""# Univariate Exploratory Data Analysis

Variabel-variabel pada RBook Recommendation Dataset dataset adalah sebagai berikut:
- Books : merupakan data mengenai buku yang mencakup ISBN, judul buku, penulis, tahun terbit, dan nama penerbit.

- Users : merupakan data mengenai pengguna yang terdiri dari User-ID, lokasi (negara/kota), dan usia pengguna.

- Ratings : merupakan data interaksi antara pengguna dan buku, berupa rating yang diberikan pengguna terhadap suatu buku dengan skala 0–10.

## Books

Menampilkan struktur dan ringkasan dataframe books, seperti jumlah entri, tipe data kolom, jumlah non-null
"""

print(books.info())

"""Memuat tiga dataset (Books.csv, Users.csv, Ratings.csv) dan:

- Mengonversi kolom Year-Of-Publication menjadi numerik (menghindari string seperti 'DK Pub').

- Menampilkan informasi ringkas (info()) dari ketiga dataset untuk verifikasi awal bentuk dan isi.
"""

# Load data
import pandas as pd

books = pd.read_csv('book_data/Books.csv', encoding='latin-1')
users = pd.read_csv('book_data/Users.csv', encoding='latin-1')
ratings = pd.read_csv('book_data/Ratings.csv', encoding='latin-1')

# Bersihkan Year-Of-Publication
books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')

# Tampilkan info dasar
print("Books")
print(books.info())
print("\nUsers")
print(users.info())
print("\nRatings")
print(ratings.info())

"""## Users

Menampilkan info struktur dataframe users.
"""

print(users.info())

"""Menampilkan jumlah user unik dan statistik ringkasan kolom Age (rata-rata, min, max, dsb)."""

print('Jumlah user unik: ', len(users['User-ID'].unique()))
print('Usia pengguna - Statistik deskriptif:')
print(users['Age'].describe())

"""## Ratings

Menampilkan struktur dataframe ratings.
"""

print(ratings.info())

"""Menampilkan 5 baris pertama data rating untuk melihat format data."""

print('Contoh rating:')
print(ratings.head())

"""Menampilkan jumlah user dan ISBN unik dan menampilkan distribusi frekuensi dari nilai Book-Rating."""

print('Jumlah user unik: ', len(ratings['User-ID'].unique()))
print('Jumlah ISBN unik: ', len(ratings['ISBN'].unique()))
print('Distribusi rating:')
print(ratings['Book-Rating'].value_counts().sort_index())

"""Visualisasi dari:
- Distribusi Rating Buku
- Top 20 User dengan Jumlah Rating Terbanyak
- Top 20 Buku yang Paling Banyak Dirating
- Distribusi Usia Pengguna
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Distribusi Book Rating
plt.figure(figsize=(8, 4))
sns.histplot(ratings['Book-Rating'], bins=10, kde=False)
plt.title('Distribusi Rating Buku')
plt.xlabel('Book Rating')
plt.ylabel('Jumlah')
plt.show()

# Jumlah interaksi per user (Top 20)
user_counts = ratings['User-ID'].value_counts().head(20)
plt.figure(figsize=(10, 4))
sns.barplot(x=user_counts.index.astype(str), y=user_counts.values, palette='Blues_d')
plt.title('Top 20 User dengan Jumlah Rating Terbanyak')
plt.xlabel('User-ID')
plt.ylabel('Jumlah Rating')
plt.xticks(rotation=45)
plt.show()

# Jumlah interaksi per buku (Top 20)
book_counts = ratings['ISBN'].value_counts().head(20)
plt.figure(figsize=(10, 4))
sns.barplot(x=book_counts.index, y=book_counts.values, palette='Greens_d')
plt.title('Top 20 Buku yang Paling Banyak Dirating')
plt.xlabel('ISBN')
plt.ylabel('Jumlah Rating')
plt.xticks(rotation=45)
plt.show()

# Distribusi usia pengguna
plt.figure(figsize=(8, 4))
sns.histplot(users['Age'], bins=30, kde=True)
plt.title('Distribusi Usia Pengguna')
plt.xlabel('Usia')
plt.ylabel('Jumlah')
plt.show()

"""# Data Preprocessing

## Cleaning Data Awal

Pada tahap ini dilakukan inisialisasi proses pembersihan data, seperti mengonversi kolom numerik dan memfilter data tidak valid sebagai langkah awal.
Proses pembersihan lanjutan secara menyeluruh akan dijelaskan dan diterapkan pada tahap Data Preparation.
"""

# Contoh awal cleaning data

# Konversi kolom 'Year-Of-Publication' menjadi numerik
books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')

# Filter user dengan usia antara 5 hingga 100 tahun
users = users[(users['Age'] >= 5) & (users['Age'] <= 100)]

# Tampilkan missing value sebagai acuan pembersihan awal
print("Missing value pada books:")
print(books.isnull().sum())

"""# Data Preparation

## Mengecek & Mengatasi Missing Value

Menampilkan jumlah nilai kosong (NaN) pada setiap kolom di ketiga dataframe (books, users, ratings) setelah proses filter awal.
"""

# Cek missing value pada dataset
print("Missing value pada books:")
print(books.isnull().sum())
print("\nMissing value pada users:")
print(users.isnull().sum())
print("\nMissing value pada ratings:")
print(ratings.isnull().sum())

"""Menghapus missing values dan membuat salinan data bersih (books_clean, users_clean, ratings_clean). Menampilkan kembali jumlah missing values setelah dibersihkan."""

# Hapus missing value dari semua dataframe
books_clean = books.dropna()
users_clean = users.dropna()
ratings_clean = ratings.dropna()

# Cek ulang
print("Setelah dibersihkan:")
print("Books:", books_clean.isnull().sum().sum())
print("Users:", users_clean.isnull().sum().sum())
print("Ratings:", ratings_clean.isnull().sum().sum())

"""Memastikan kembali bahwa data benar-benar bebas dari nilai kosong."""

# Mengecek kembali missing value pada dataset yang sudah dibersihkan
print("Missing value pada books_clean:")
print(books_clean.isnull().sum())

print("\nMissing value pada users_clean:")
print(users_clean.isnull().sum())

print("\nMissing value pada ratings_clean:")
print(ratings_clean.isnull().sum())

"""## Membersihkan & Memfilter Data yang Tidak Valid

Bersihkan Ulang Tahun & Usia
- Validasi ulang kolom Year-Of-Publication agar berada dalam range wajar (1800–2025).

- Memfilter ulang kolom Age agar hanya dalam 5–100 tahun.
"""

# Bersihkan kolom tahun publikasi
books_clean['Year-Of-Publication'] = pd.to_numeric(books_clean['Year-Of-Publication'], errors='coerce')
books_clean = books_clean[(books_clean['Year-Of-Publication'] >= 1800) & (books_clean['Year-Of-Publication'] <= 2025)]

# Filter usia user yang valid
users_clean = users_clean[(users_clean['Age'] >= 5) & (users_clean['Age'] <= 100)]

"""## Menyaring Data Aktif (User & Buku yang Sering Digunakan)

Filter ulang dataset bersih agar hanya menyertakan user dan buku dengan ≥10 interaksi (agar model nanti lebih andal).
"""

# Hanya ambil user dan buku yang punya setidaknya 10 interaksi
user_counts = ratings_clean['User-ID'].value_counts()
book_counts = ratings_clean['ISBN'].value_counts()

active_users = user_counts[user_counts >= 10].index
popular_books = book_counts[book_counts >= 10].index

ratings_filtered = ratings_clean[
    (ratings_clean['User-ID'].isin(active_users)) &
    (ratings_clean['ISBN'].isin(popular_books))
]

"""## Menyusun Dataframe Final untuk Modeling

- Menggabungkan ratings_filtered dengan books_clean berdasarkan ISBN.

- Menghapus baris hasil merge yang tidak memiliki Book-Title.

- Menampilkan ukuran data hasil merge sebelum dan sesudah dibersihkan.
"""

# Gabung ratings data dan book information
all_book_info = pd.merge(ratings_filtered, books_clean, on='ISBN')

all_book_info_clean = all_book_info.dropna(subset=['Book-Title'])

print(f"Original merged data shape: {all_book_info.shape}")
print(f"Cleaned merged data shape: {all_book_info_clean.shape}")

"""## Menyiapkan Data untuk Model Collaborative Filtering

- Menginstal library scikit-surprise untuk membangun sistem rekomendasi.

- Mengimpor Dataset dan Reader dari Surprise.

- Membentuk dataset Surprise dari ratings_filtered.

- Verifikasi bahwa dataset berhasil dibuat.
"""

!pip install scikit-surprise

# Import surprise
from surprise import Dataset, Reader

# Format data untuk Surprise
reader = Reader(rating_scale=(0, 10))
data = Dataset.load_from_df(ratings_filtered[['User-ID', 'ISBN', 'Book-Rating']], reader)

# Cek berhasil
print("Surprise dataset loaded successfully.")

"""# Model Development dengan Collaborative Filtering

- Mengimpor library yang dibutuhkan: pandas dan numpy untuk manipulasi data, TensorFlow untuk membangun model, matplotlib untuk visualisasi.

- Mengecek versi TensorFlow dan NumPy untuk memastikan kompatibilitas.
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

print("TensorFlow version:", tf.__version__)
print("NumPy version:", np.__version__)

"""## Data Preparation

- Menyalin dataset hasil filter.

- Meng-encode kolom User-ID dan ISBN menjadi ID numerik agar bisa digunakan dalam layer embedding.

- Menyimpan jumlah total user dan buku yang unik.

- Mengonversi Book-Rating ke tipe float32 untuk kompatibilitas model.
"""

# Ambil dataframe hasil filter
df = ratings_filtered.copy()

# Encode User-ID dan ISBN sebagai index numerik untuk embedding
user_ids = df['User-ID'].unique().tolist()
book_ids = df['ISBN'].unique().tolist()

user2user_encoded = {x: i for i, x in enumerate(user_ids)}
book2book_encoded = {x: i for i, x in enumerate(book_ids)}

df['user'] = df['User-ID'].map(user2user_encoded)
df['book'] = df['ISBN'].map(book2book_encoded)

num_users = len(user2user_encoded)
num_books = len(book2book_encoded)

df['Book-Rating'] = df['Book-Rating'].astype(np.float32)

"""## Membagi Data untuk Training dan Validasi

- Memisahkan data menjadi fitur (user, book) dan target (Book-Rating).

- Membagi dataset menjadi training dan validation set sebesar 80:20.
"""

from sklearn.model_selection import train_test_split

x = df[['user', 'book']].values
y = df['Book-Rating'].values

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""## Proses Training Model

- Menormalisasi rating ke rentang 0–1 agar proses pelatihan model lebih stabil.

- Menentukan parameter penting seperti jumlah user, buku, dan ukuran embedding.
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from sklearn.preprocessing import MinMaxScaler

# Normalisasi Target Rating
scaler = MinMaxScaler(feature_range=(0, 1))
y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
y_val_scaled = scaler.transform(y_val.reshape(-1, 1)).flatten()

# Parameter Dasar
num_users = len(user2user_encoded)
num_books = len(book2book_encoded)
embedding_size = 32
reg_strength = 1e-6
dropout_rate = 0.4

"""- Membangun input dan layer embedding untuk user dan buku, menggabungkan representasi user dan buku menjadi satu vektor.
- Membangun hidden layer dengan dropout untuk regularisasi. Layer output menggunakan aktivasi linear karena target sudah dinormalisasi.
"""

# Input Layer
user_input = Input(shape=(1,), name='user_input')
book_input = Input(shape=(1,), name='book_input')

# Embedding Layer
user_embedding = Embedding(
    input_dim=num_users,
    output_dim=embedding_size,
    name='user_embedding',
    embeddings_regularizer=l2(reg_strength)
)(user_input)

book_embedding = Embedding(
    input_dim=num_books,
    output_dim=embedding_size,
    name='book_embedding',
    embeddings_regularizer=l2(reg_strength)
)(book_input)

# Flatten Embedding
user_vec = Flatten()(user_embedding)
book_vec = Flatten()(book_embedding)

# Concatenate + Hidden Layers (64 → 32)
concat = Concatenate()([user_vec, book_vec])
x = BatchNormalization()(concat)
x = Dense(64, activation='relu', kernel_regularizer=l2(reg_strength))(x)
x = Dropout(dropout_rate)(x)
x = BatchNormalization()(x)
x = Dense(32, activation='relu', kernel_regularizer=l2(reg_strength))(x)
x = Dropout(dropout_rate)(x)

# Output Layer
output = Dense(1, activation='linear')(x)

# Compile Model
model = Model(inputs=[user_input, book_input], outputs=output)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, amsgrad=True),
    loss='mse',
    metrics=['mae']
)

# Callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)

"""Melatih model menggunakan data training dan memantau performa pada validation set."""

# Training
history = model.fit(
    x=[x_train[:, 0], x_train[:, 1]],
    y=y_train_scaled,
    validation_data=([x_val[:, 0], x_val[:, 1]], y_val_scaled),
    epochs=40,
    batch_size=64,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# Evaluasi Akhir di Test Set
test_loss, test_mae = model.evaluate(
    x=[x_val[:, 0], x_val[:, 1]],
    y=y_val_scaled,
    verbose=1
)
print(f"\n Validation Loss: {test_loss:.4f}, Validation MAE: {test_mae:.4f}")

"""## Visualisasi Metrik

Menampilkan grafik MSE loss selama proses training untuk memantau overfitting/underfitting.
"""

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.show()

"""## Diagnosa Overfitting / Underfitting

Untuk mengevaluasi apakah model mengalami overfitting atau underfitting, dilakukan analisis terhadap nilai loss dan MAE pada data pelatihan dan validasi. Jika selisih nilai pada data validasi dan pelatihan cukup besar, itu bisa mengindikasikan overfitting.

"""

import matplotlib.pyplot as plt

# Plot loss
plt.figure(figsize=(12, 5))

# Plot loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot MAE
plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Val MAE')
plt.title('MAE per Epoch')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

plt.tight_layout()
plt.show()

"""## Evaluasi Model (Denormalisasi + RMSE/MAE Akhir)

- Melakukan prediksi pada data validasi.

- Mengembalikan hasil prediksi dan ground truth ke skala asli.

- Menghitung metrik RMSE dan MAE sebagai evaluasi performa.
"""

# Prediksi pada data validasi
y_pred_scaled = model.predict([x_val[:, 0], x_val[:, 1]])
y_pred = scaler.inverse_transform(y_pred_scaled)

# Denormalisasi ground truth
y_true = scaler.inverse_transform(y_val_scaled.reshape(-1, 1))

# Hitung metrik evaluasi akhir
from sklearn.metrics import mean_squared_error, mean_absolute_error

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae = mean_absolute_error(y_true, y_pred)

print(f"Final RMSE: {rmse:.4f}")
print(f"Final MAE: {mae:.4f}")

"""## Load Data & Fungsi Pembalik Encoding

- Membuat dictionary pembalik agar bisa mengubah ID numerik kembali ke user ID asli dan ISBN.

- Membuat mapping dari ISBN ke judul buku.
"""

# Mapping pembalik untuk user dan buku
user_encoded2user = {v: k for k, v in user2user_encoded.items()}
book_encoded2book = {v: k for k, v in book2book_encoded.items()}

# Untuk mencari judul buku dari ISBN
isbn2title = dict(zip(books['ISBN'], books['Book-Title']))

"""## Membuat Fungsi Rekomendasi Buku

- Fungsi ini menerima user_id dan memprediksi rating untuk semua buku yang belum pernah dirating oleh user tersebut.

- Hasil prediksi diurutkan dan ditampilkan sebagai rekomendasi teratas.
"""

def recommend_books_for_user(user_id, model, df, user2user_encoded, book_encoded2book, isbn2title, scaler, num_books, top_n=10):
    # Pastikan user tersedia dalam data training
    if user_id not in user2user_encoded:
        print("User tidak ditemukan dalam data latih.")
        return []

    # Encoding user ID
    encoded_user_id = user2user_encoded[user_id]

    # Buku-buku yang sudah diberi rating oleh user
    book_ids_already_rated = df[df['user'] == encoded_user_id]['book'].tolist()

    # Daftar kandidat buku yang belum dirating user
    candidate_books = np.setdiff1d(np.arange(num_books), book_ids_already_rated)

    # Input prediksi
    user_input_array = np.array([encoded_user_id] * len(candidate_books))
    book_input_array = candidate_books

    # Prediksi rating berskala 0–1, lalu dikembalikan ke skala asli
    pred_ratings_scaled = model.predict([user_input_array, book_input_array], verbose=0)
    pred_ratings = scaler.inverse_transform(pred_ratings_scaled).flatten()

    # Ambil Top-N buku dengan rating prediksi tertinggi, kecuali yang judulnya "Unknown Title"
    top_indices = np.argsort(pred_ratings)[::-1]
    recommendations = []

    for idx in top_indices:
        book_id = candidate_books[idx]
        rating = pred_ratings[idx]
        isbn = book_encoded2book[book_id]
        title = isbn2title.get(isbn, "Unknown Title")

        if title != "Unknown Title":
            recommendations.append((isbn, title, round(rating, 2)))

        if len(recommendations) == top_n:
            break

    return recommendations

"""## Contoh Penggunaan

Memilih satu user dari data dan menghasilkan rekomendasi untuk user tersebut, mencetak hasilnya dalam bentuk daftar buku dan prediksi rating.
"""

# Mapping dari book encoded ke ISBN
book_encoded2book = {i: isbn for i, isbn in enumerate(book_ids)}

# Mapping dari ISBN ke judul buku
isbn2title = books_clean.set_index('ISBN')['Book-Title'].to_dict()

# Contoh rekomendasi untuk satu user
user_id_example = 276798  # Bisa ganti dengan user valid dari dataset
recs = recommend_books_for_user(
    user_id=user_id_example,
    model=model,
    df=df,
    user2user_encoded=user2user_encoded,
    book_encoded2book=book_encoded2book,
    isbn2title=isbn2title,
    scaler=scaler,
    num_books=num_books,
    top_n=10
)

# Tampilkan hasil rekomendasi
print(f"Rekomendasi buku untuk User-ID {user_id_example}:\n")
for i, (isbn, title, pred_rating) in enumerate(recs, 1):
    print(f"{i}. {title} (ISBN: {isbn}) — Prediksi Rating: {pred_rating}")

"""## Kesimpulan & Insight Akhir

1. **Sistem rekomendasi berhasil dibangun** menggunakan pendekatan collaborative filtering berbasis embedding neural network.
2. **Model menunjukkan performa yang memadai**, dengan RMSE 3.4196 dan MAE 2.7997.
3. **Rekomendasi yang dihasilkan dipersonalisasi** berdasarkan interaksi historis pengguna dan buku.
4. **Sistem membantu pengguna menemukan bacaan baru** tanpa eksplorasi manual.
5. **Problem Statement berhasil dijawab**:
   - Sistem dapat memprediksi dan merekomendasikan buku yang relevan.
   - Memberikan nilai tambah dalam menjelajahi buku baru.
6. **Potensi pengembangan selanjutnya**:
   - Integrasi metadata buku (judul, genre, penulis).
   - Penerapan model hybrid (collaborative + content-based).
   - Eksperimen dengan model advanced seperti Neural Collaborative Filtering (NCF).
"""